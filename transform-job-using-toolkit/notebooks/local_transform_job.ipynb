{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "724dd4d0-df29-46c2-8b82-ed53a368cd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sagemaker.local import LocalSession\n",
    "\n",
    "sagemaker_session = LocalSession()\n",
    "sagemaker_session.config = {'local': {'local_code': True}}\n",
    "\n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = \"us-east-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad13d592-9ab8-4411-a7d1-9c401fcbe79c",
   "metadata": {},
   "source": [
    "# Set Up docker and .tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ab03390d-def9-4650-8ccb-035dd47c3341",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building docker container with name transform_job\n",
      "Sending build context to Docker daemon  14.34kB\n",
      "Step 1/17 : FROM ubuntu:18.04\n",
      " ---> 5d2df19066ac\n",
      "Step 2/17 : LABEL com.amazonaws.sagemaker.capabilities.multi-models=true\n",
      " ---> Using cache\n",
      " ---> 8ac2d20bd209\n",
      "Step 3/17 : LABEL com.amazonaws.sagemaker.capabilities.accept-bind-to-port=true\n",
      " ---> Using cache\n",
      " ---> b7eaa0f56164\n",
      "Step 4/17 : RUN apt-get update && apt-get upgrade -y && apt-get clean\n",
      " ---> Using cache\n",
      " ---> 8c5bb3da17a4\n",
      "Step 5/17 : RUN apt-get install -y curl python3.7 python3.7-dev python3.7-distutils\n",
      " ---> Using cache\n",
      " ---> e762fbc99c8c\n",
      "Step 6/17 : RUN update-alternatives --install /usr/bin/python python /usr/bin/python3.7 1\n",
      " ---> Using cache\n",
      " ---> 7998b6bb7638\n",
      "Step 7/17 : RUN update-alternatives --set python /usr/bin/python3.7\n",
      " ---> Using cache\n",
      " ---> 3ec8e511d35c\n",
      "Step 8/17 : RUN apt-get -y install --no-install-recommends     build-essential     ca-certificates     openjdk-8-jdk-headless     curl     vim     && rm -rf /var/lib/apt/lists/*     && python --version     && curl -O https://bootstrap.pypa.io/get-pip.py     && python get-pip.py\n",
      " ---> Using cache\n",
      " ---> e55ed7293025\n",
      "Step 9/17 : RUN update-alternatives --install /usr/bin/python python /usr/bin/python3 1\n",
      " ---> Using cache\n",
      " ---> 0ab8c196602e\n",
      "Step 10/17 : RUN update-alternatives --install /usr/local/bin/pip pip /usr/local/bin/pip3 1\n",
      " ---> Using cache\n",
      " ---> f55249e54d65\n",
      "Step 11/17 : RUN pip3 --no-cache-dir install mxnet                                 multi-model-server                                 sagemaker-inference                                 retrying\n",
      " ---> Using cache\n",
      " ---> cc4f7ef9e595\n",
      "Step 12/17 : COPY dockerd-entrypoint.py /usr/local/bin/dockerd-entrypoint.py\n",
      " ---> Using cache\n",
      " ---> b70a2e6cf932\n",
      "Step 13/17 : RUN chmod +x /usr/local/bin/dockerd-entrypoint.py\n",
      " ---> Using cache\n",
      " ---> 4d4141728bf9\n",
      "Step 14/17 : ENV PYTHONDONTWRITEBYTECODE=1\n",
      " ---> Running in 3afe07873272\n",
      "Removing intermediate container 3afe07873272\n",
      " ---> 040ced0df9d6\n",
      "Step 15/17 : EXPOSE 8080\n",
      " ---> Running in d0b93fbde57e\n",
      "Removing intermediate container d0b93fbde57e\n",
      " ---> 96482436749b\n",
      "Step 16/17 : ENTRYPOINT [\"python\", \"/usr/local/bin/dockerd-entrypoint.py\"]\n",
      " ---> Running in 893b1c6e3ef1\n",
      "Removing intermediate container 893b1c6e3ef1\n",
      " ---> 6de9ff63a7e5\n",
      "Step 17/17 : CMD [\"serve\"]\n",
      " ---> Running in fe242976fcc7\n",
      "Removing intermediate container fe242976fcc7\n",
      " ---> 2eed1da6ca52\n",
      "Successfully built 2eed1da6ca52\n",
      "Successfully tagged transform_job:latest\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd .. && ./build.sh ./Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f053f831-d94d-40bc-a3a2-b1137cf4c84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code/inference.py\n",
      "code/requirements.txt\n",
      "model.json\n",
      "-rw-rw-r-- manu-lasker26/manu-lasker26 582 2023-01-31 23:06 code/inference.py\n",
      "-rw-rw-r-- manu-lasker26/manu-lasker26   6 2023-01-31 21:59 code/requirements.txt\n",
      "-rw-rw-r-- manu-lasker26/manu-lasker26  30 2023-01-31 21:36 model.json\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd .. && tar -czvf artifact.tar.gz code/* model.json && tar -ztvf artifact.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636d7c08-301a-4b5b-8b7c-330879f02e34",
   "metadata": {},
   "source": [
    "# Set up local Transform job and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e4dea465-d909-4cb9-8fe2-29b3e5cd0e15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'home/manu-lasker26/Documents/manulasker/personal_projects/sagemaker-transform-job-inference/transform-job'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory = os.path.join(*os.getcwd().split(os.sep)[:-1])\n",
    "directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e4002bcc-09d5-447f-b361-c6a6f99dc31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.transformer import Transformer\n",
    "from sagemaker.model import Model\n",
    "\n",
    "env_variables = {\n",
    "    \"SAGEMAKER_PROGRAM\": \"inference.py\"\n",
    "}\n",
    "\n",
    "model = Model(\n",
    "    image_uri = \"transform_job\",\n",
    "    model_data = f\"file:///{directory}/artifact.tar.gz\",\n",
    "    role = \"arn:aws:iam::12345678910:role/test_role\",\n",
    "    env = env_variables,\n",
    "    name = \"test_model\",\n",
    "    sagemaker_session = sagemaker_session\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "fdcb0900-91ad-4a6b-9aa0-b586e3c2cee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: test_model\n"
     ]
    }
   ],
   "source": [
    "model.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "6ccda0c2-f194-4fb5-82b1-a3baade81e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Transformer(\n",
    "    model_name = model.name,\n",
    "    instance_count = 1,\n",
    "    instance_type = \"local\",\n",
    "    output_path = f\"file:///{directory}/output\",\n",
    "    sagemaker_session = sagemaker_session,\n",
    "    env = env_variables\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "1d4bc5be-c363-4085-b951-96051d309564",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating transform job with name: transform_job-2023-02-01-04-16-20-678\n",
      "INFO:sagemaker.local.image:serving\n",
      "INFO:sagemaker.local.image:creating hosting dir in /tmp/tmpjhxr1evh\n",
      "INFO:botocore.credentials:Found credentials in environment variables.\n",
      "WARNING:sagemaker.local.image:Using the short-lived AWS credentials found in session. They might expire while running.\n",
      "INFO:sagemaker.local.image:docker compose file: \n",
      "networks:\n",
      "  sagemaker-local:\n",
      "    name: sagemaker-local\n",
      "services:\n",
      "  algo-1-lw9gs:\n",
      "    command: serve\n",
      "    container_name: 4hym5pj681-algo-1-lw9gs\n",
      "    environment:\n",
      "    - '[Masked]'\n",
      "    - '[Masked]'\n",
      "    - '[Masked]'\n",
      "    - '[Masked]'\n",
      "    - '[Masked]'\n",
      "    - '[Masked]'\n",
      "    image: transform_job\n",
      "    networks:\n",
      "      sagemaker-local:\n",
      "        aliases:\n",
      "        - algo-1-lw9gs\n",
      "    ports:\n",
      "    - 8080:8080\n",
      "    stdin_open: true\n",
      "    tty: true\n",
      "    volumes:\n",
      "    - /home/manu-lasker26/Documents/manulasker/personal_projects/sagemaker-transform-job-inference/transform-job:/opt/ml/model\n",
      "version: '2.3'\n",
      "\n",
      "INFO:sagemaker.local.image:docker command: docker-compose -f /tmp/tmpjhxr1evh/docker-compose.yaml up --build --abort-on-container-exit\n",
      "INFO:sagemaker.local.entities:Checking if serving container is up, attempt: 5\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fda20a04a90>: Failed to establish a new connection: [Errno 111] Connection refused')': /ping\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fda20a04fa0>: Failed to establish a new connection: [Errno 111] Connection refused')': /ping\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd9f87e11f0>: Failed to establish a new connection: [Errno 111] Connection refused')': /ping\n",
      "INFO:sagemaker.local.entities:Container still not up, got: -1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attaching to 4hym5pj681-algo-1-lw9gs\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m Collecting pandas\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m   Downloading pandas-1.3.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m:--:--\u001b[0m\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m \u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas->-r /opt/ml/model/code/requirements.txt (line 1)) (1.21.6)\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m Collecting python-dateutil>=2.7.3\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m   Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.7/247.7 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m \u001b[?25hCollecting pytz>=2017.3\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m   Downloading pytz-2022.7.1-py2.py3-none-any.whl (499 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.4/499.4 kB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m \u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->-r /opt/ml/model/code/requirements.txt (line 1)) (1.16.0)\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m Installing collected packages: pytz, python-dateutil, pandas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.local.entities:Checking if serving container is up, attempt: 10\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /ping\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /ping\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /ping\n",
      "INFO:sagemaker.local.entities:Container still not up, got: -1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m Successfully installed pandas-1.3.5 python-dateutil-2.8.2 pytz-2022.7.1\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m \u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m \u001b[0mWarning: MMS is using non-default JVM parameters: -XX:-UseContainerSupport\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m 2023-02-01T04:16:30,222 [INFO ] main com.amazonaws.ml.mms.ModelServer - \n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m MMS Home: /usr/local/lib/python3.7/dist-packages\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m Current directory: /\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m Temp directory: /tmp\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m Number of GPUs: 0\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m Number of CPUs: 8\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m Max heap size: 3525 M\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m Python executable: /usr/bin/python\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m Config file: /etc/sagemaker-mms.properties\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m Inference address: http://0.0.0.0:8080\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m Management address: http://0.0.0.0:8080\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m Model Store: /\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m Initial Models: model=/opt/ml/model\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m Log dir: null\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m Metrics dir: null\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m Netty threads: 0\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m Netty client threads: 0\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m Default workers per model: 8\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m Blacklist Regex: N/A\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m Maximum Response Size: 6553500\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m Maximum Request Size: 6553500\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m Preload model: false\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m Prefer direct buffer: false\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m 2023-02-01T04:16:30,227 [INFO ] main com.amazonaws.ml.mms.ModelServer - Loading initial models: /opt/ml/model preload_model: false\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m 2023-02-01T04:16:30,260 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-9000-model\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m 2023-02-01T04:16:30,308 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - model_service_worker started with args: --sock-type unix --sock-name /tmp/.mms.sock.9000 --handler sagemaker_inference.default_handler_service --model-path /opt/ml/model --model-name model --preload-model false --tmp-dir /tmp\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m 2023-02-01T04:16:30,309 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /tmp/.mms.sock.9000\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m 2023-02-01T04:16:30,310 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID] 48\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m 2023-02-01T04:16:30,310 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MMS worker started.\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m 2023-02-01T04:16:30,310 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.7.5\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m 2023-02-01T04:16:30,310 [INFO ] main com.amazonaws.ml.mms.wlm.ModelManager - Model model loaded.\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m 2023-02-01T04:16:30,315 [INFO ] main com.amazonaws.ml.mms.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m 2023-02-01T04:16:30,322 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /tmp/.mms.sock.9000\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m 2023-02-01T04:16:30,322 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /tmp/.mms.sock.9000\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m 2023-02-01T04:16:30,322 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /tmp/.mms.sock.9000\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m 2023-02-01T04:16:30,322 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /tmp/.mms.sock.9000\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m 2023-02-01T04:16:30,322 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /tmp/.mms.sock.9000\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m 2023-02-01T04:16:30,322 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /tmp/.mms.sock.9000\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m 2023-02-01T04:16:30,322 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /tmp/.mms.sock.9000\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m 2023-02-01T04:16:30,322 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /tmp/.mms.sock.9000\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m 2023-02-01T04:16:30,387 [INFO ] main com.amazonaws.ml.mms.ModelServer - Inference API bind to: http://0.0.0.0:8080\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m 2023-02-01T04:16:30,388 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /tmp/.mms.sock.9000.\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m Model server started.\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m 2023-02-01T04:16:30,390 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /tmp/.mms.sock.9000.\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m 2023-02-01T04:16:30,391 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /tmp/.mms.sock.9000.\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m 2023-02-01T04:16:30,393 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /tmp/.mms.sock.9000.\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m 2023-02-01T04:16:30,395 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /tmp/.mms.sock.9000.\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m 2023-02-01T04:16:30,398 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /tmp/.mms.sock.9000.\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m 2023-02-01T04:16:30,400 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /tmp/.mms.sock.9000.\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m 2023-02-01T04:16:30,400 [WARN ] pool-3-thread-1 com.amazonaws.ml.mms.metrics.MetricCollector - worker pid is not available yet.\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m 2023-02-01T04:16:30,403 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /tmp/.mms.sock.9000.\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m 2023-02-01T04:16:31,283 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Model model loaded io_fd=0242acfffe120002-00000019-00000004-c159647a4dd0c279-11b2eb9b\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m 2023-02-01T04:16:31,287 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Model model loaded io_fd=0242acfffe120002-00000019-00000001-6a4da47a4dd0c279-8aff1831\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m 2023-02-01T04:16:31,293 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Model model loaded io_fd=0242acfffe120002-00000019-00000007-8de5e47a4dd0c279-da58f185\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m 2023-02-01T04:16:31,296 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Model model loaded io_fd=0242acfffe120002-00000019-00000002-70d3a47a4dd0c279-cd5e1c6a\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m 2023-02-01T04:16:31,296 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Model model loaded io_fd=0242acfffe120002-00000019-00000000-5e35a47a4dd0c279-3b715e94\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m 2023-02-01T04:16:31,298 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 829\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m 2023-02-01T04:16:31,298 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 833\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m 2023-02-01T04:16:31,298 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 827\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m 2023-02-01T04:16:31,298 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 833\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m 2023-02-01T04:16:31,298 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 811\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m 2023-02-01T04:16:31,300 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Model model loaded io_fd=0242acfffe120002-00000019-00000005-6815647a4dd0c279-6248ad44\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m 2023-02-01T04:16:31,301 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 825\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m 2023-02-01T04:16:31,301 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-model-8\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m 2023-02-01T04:16:31,301 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-model-3\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m 2023-02-01T04:16:31,301 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-model-7\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m 2023-02-01T04:16:31,301 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-model-4\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m 2023-02-01T04:16:31,301 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-model-2\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m 2023-02-01T04:16:31,302 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-model-1\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m 2023-02-01T04:16:31,308 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Model model loaded io_fd=0242acfffe120002-00000019-00000006-4806e47a4dd0c279-f6b55964\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m 2023-02-01T04:16:31,308 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 843\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m 2023-02-01T04:16:31,309 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-model-6\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m 2023-02-01T04:16:31,317 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Model model loaded io_fd=0242acfffe120002-00000019-00000003-db27a47a4dd0c279-58279aad\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m 2023-02-01T04:16:31,318 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 825\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m 2023-02-01T04:16:31,318 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-model-5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.local.entities:Checking if serving container is up, attempt: 15\n",
      "INFO:root:copying /tmp/tmpersmgka5/data_1.json.out -> /home/manu-lasker26/Documents/manulasker/personal_projects/sagemaker-transform-job-inference/transform-job/output\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m 2023-02-01T04:16:32,815 [INFO ] pool-2-thread-10 ACCESS_LOG - /172.18.0.1:53458 \"GET /ping HTTP/1.1\" 200 7\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m 2023-02-01T04:16:32,852 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /172.18.0.1:53466 \"GET /execution-parameters HTTP/1.1\" 404 1\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m 2023-02-01T04:16:32,891 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 1\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m 2023-02-01T04:16:32,891 [INFO ] W-model-4-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - {'data_number': '1'}\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m 2023-02-01T04:16:32,891 [WARN ] W-model-4-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - 2023-02-01 04:16:32,890 - sagemaker-inference - INFO - {'data_number': '1'}\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m 2023-02-01T04:16:32,891 [INFO ] W-model-4-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - {'model': 'hola mundo'}\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m 2023-02-01T04:16:32,891 [WARN ] W-model-4-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - 2023-02-01 04:16:32,890 - sagemaker-inference - INFO - {'model': 'hola mundo'}\n",
      "\u001b[36m4hym5pj681-algo-1-lw9gs |\u001b[0m 2023-02-01T04:16:32,891 [INFO ] W-9000-model ACCESS_LOG - /172.18.0.1:53478 \"POST /invocations HTTP/1.1\" 200 3\n",
      "Gracefully stopping... (press Ctrl+C again to force)\n",
      "."
     ]
    }
   ],
   "source": [
    "transform.transform(data = f\"file:///{directory}/data/\", content_type=\"application/json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463274e5-0509-4639-9fa8-a93a1f925504",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c370523d-d6f4-498e-8c96-294f40f605d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"data_number\": \"1\"}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "out_files = [file for file in os.listdir(f\"/{directory}/output/\") if file.endswith(\"out\")]\n",
    "\n",
    "for out_file in out_files:\n",
    "    with open(f\"/{directory}/output/{out_file}\", \"r\") as file:\n",
    "        print(json.dumps(json.load(file)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce99ba02-dd81-411a-998a-5ba59841877b",
   "metadata": {},
   "source": [
    "# Clean Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0b3833cf-978c-42fc-8f94-0d66c7f5e482",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd .. && rm artifact.tar.gz && rm -r output/*.out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
